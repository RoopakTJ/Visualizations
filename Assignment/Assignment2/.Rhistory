cat("Accuracy of test data with new model\t\t", accuracy_new)
cat("\n")
cat("RMSE test data with old model:\t\t\t", root_mean_square_new)
cat("\n")
cat("Negative Log Likelihood test data with old model :\t\t\t", nll_new)
cat("\n")
library(Metrics)
sensitivity = confusion.matrix1[2,2]/(confusion.matrix1[2,1] + confusion.matrix1[2,2])
specificity = confusion.matrix1[1,1]/(confusion.matrix1[1,1] + confusion.matrix1[1,2])
accuracy_old = mean(round(pld_old) == ilt$Dataset)
root_mean_square_old = rmse(ilt$Dataset, round(pld_old))
nll_old = negative.log.likelihood(pld_old, ilt$Dataset)
cat("--------------- Previous model with complete features-------------------")
cat("\n")
cat("Sensitivity of test data with old model:\t", sensitivity)
cat("\n")
cat("Specificity of test data with old model:\t", specificity)
cat("\n")
cat("Accuracy of test data with old model:\t\t", accuracy_old)
cat("\n")
cat("RMSE test data with old model:\t\t\t", root_mean_square_old)
cat("\n")
cat("Negative Log Likelihood test data with old model :\t", nll_old)
cat("\n")
sensitivity = confusion.matrix2[2,2]/(confusion.matrix2[2,1] + confusion.matrix2[2,2])
specificity = confusion.matrix1[1,1]/(confusion.matrix2[1,1] + confusion.matrix2[1,2])
accuracy_new = mean(round(pld_new) == ilt$Dataset)
root_mean_square_new = rmse(ilt$Dataset, round(pld_new))
nll_new = negative.log.likelihood(pld_new, ilt$Dataset)
cat("\n")
cat("--------------- New model with selected features-------------------")
cat("\n")
cat("Sensitivity of test data with new model\t\t", sensitivity)
cat("\n")
cat("Specificity of test data with new model\t\t", specificity)
cat("\n")
cat("Accuracy of test data with new model\t\t", accuracy_new)
cat("\n")
cat("RMSE test data with old model:\t\t\t", root_mean_square_new)
cat("\n")
cat("Negative Log Likelihood test data with old model :\t", nll_new)
cat("\n")
library(Metrics)
sensitivity = confusion.matrix1[2,2]/(confusion.matrix1[2,1] + confusion.matrix1[2,2])
specificity = confusion.matrix1[1,1]/(confusion.matrix1[1,1] + confusion.matrix1[1,2])
accuracy_old = mean(round(pld_old) == ilt$Dataset)
root_mean_square_old = rmse(ilt$Dataset, round(pld_old))
nll_old = negative.log.likelihood(pld_old, ilt$Dataset)
cat("--------------- Previous model with complete features-------------------")
cat("\n")
cat("Sensitivity of test data with old model:\t", sensitivity)
cat("\n")
cat("Specificity of test data with old model:\t", specificity)
cat("\n")
cat("Accuracy of test data with old model:\t\t", accuracy_old)
cat("\n")
cat("RMSE test data with old model:\t\t\t", root_mean_square_old)
cat("\n")
cat("Negative Log Likelihood test data with old model :", nll_old)
cat("\n")
sensitivity = confusion.matrix2[2,2]/(confusion.matrix2[2,1] + confusion.matrix2[2,2])
specificity = confusion.matrix1[1,1]/(confusion.matrix2[1,1] + confusion.matrix2[1,2])
accuracy_new = mean(round(pld_new) == ilt$Dataset)
root_mean_square_new = rmse(ilt$Dataset, round(pld_new))
nll_new = negative.log.likelihood(pld_new, ilt$Dataset)
cat("\n")
cat("--------------- New model with selected features-------------------")
cat("\n")
cat("Sensitivity of test data with new model\t\t", sensitivity)
cat("\n")
cat("Specificity of test data with new model\t\t", specificity)
cat("\n")
cat("Accuracy of test data with new model\t\t", accuracy_new)
cat("\n")
cat("RMSE test data with old model:\t\t\t", root_mean_square_new)
cat("\n")
cat("Negative Log Likelihood test data with old model :", nll_new)
cat("\n")
fit_3 <- lm(Total_Proteins ~ ., data=il)
summary(fit_3)
predict.Total_Protiens <- predict(fit_3, type = "response")
x <- seq(0:394)
y1 <- il$Total_Proteins
y2 <- predict.Total_Protiens
df <- data.frame(x,y1,y2)
ggplot(df, aes(y1, y2)) +                    # basic graphical object
geom_point(aes(y1), colour="green")+# first layer
xlab("Truth Value") +
ylab("Predicted value") +
ggtitle("Truth vs Predicted - Total Protiens") +
theme_dark()
predict.Total_Protiens <- predict(fit_3, type = "response")
x <- seq(0:394)
y1 <- il$Total_Proteins
y2 <- predict.Total_Protiens
df <- data.frame(x,y1,y2)
ggplot(df, aes(y1, y2)) +                    # basic graphical object
geom_point(aes(y1), colour="green")+# first layer
xlab("Truth Value") +
ylab("Predicted value") +
ggtitle("Truth vs Predicted - Total Protiens") +
theme_dark()
cor(il[,c(1,3,4,5,6,7,8,9,10,11)])
sum.fit <- summary(fit_3)
coef(sum.fit)
# here is a DUMMY random function to select a feature to add
# => you should write a better one!
selectAddVar <- function(fset, fit, sum.fit) {
# Storing the adjusted R2 of the input fit
adjRSq <- sum.fit$adj.r.squared
maxR <- 0
maxvar <- "STOP"
# fset = the existing set of features to select from
if (length(fset)==0) {
return("STOP")
}
for ( var in fset ) {
if ( startsWith(var, "(Inter") ){
next
}
# Get the existing column names in the building model
a <- attr(coef(sum.fit), "dimnames")[[1]]
a <- a[a!="(Intercept)"] # Intercept will get added each time, need to remove
a <- gsub("GenderMale", "Gender", a) # Substitude GenderMale with Gender
fit.test <- lm(paste("Total_Proteins ~ ", paste(append(a,var),collapse="+")), data=il) # Performing lm for each variable
sum.test <- summary(fit.test)
if (sum.test$adj.r.squared > adjRSq){
maxR <- sum.test$adj.r.squared # Replace with the existing adj R squared if value is greater
maxvar <- var
}
}
#your method to select the best feature here
return(maxvar)
}
# this is the active predictor list
nil <- names(il)
# delete the target variables from set
nil <- nil[(!nil=="Total_Proteins")]
print(paste("STARTING: ", paste(nil,collapse="+")))
# start with the empty set of features for lm()
fset <- 1
# now run with initial list
fit <- lm(paste("Total_Proteins ~ ",paste(fset,collapse="+")), data=il)
sum.fit <- summary(fit)
for (loop in 1:10) {
if ( length(nil)==0 ) {
# quit if none more to add
break
}
#var is to be tested
var <- selectAddVar(nil,fit,sum.fit)
if ( var == "STOP") {
# quit as told to stop
break
}
# remove from list
nil <- nil[!nil==var]
fset <- append(fset,var)
print(paste("ADDED: ", var))
print("----------------------------------------------------------" )
# report
print(paste("RUNNING: ", paste(fset,collapse="+")))
# now run with modified list
fit <- lm(paste("Total_Proteins ~ ",paste(fset,collapse="+")), data=il)
sum.fit <- summary(fit)
print(paste("LM Rsquared out: ", sum.fit$r.squared))
}
# report the final fit
summary(fit)
# Old model
attr(coef(summary(fit_3)),"dimnames")[[1]]
# Old model
attr(coef(summary(fit)),"dimnames")[[1]]
ilt$Dataset[ilt$Dataset == 2] <- 0
# general function for this
pld_train <- predict(fit, il, type="response")
pld_old <- predict(fit_3, ilt, type="response")
pld_new <- predict(fit, ilt, type = "response")
pld_train <- predict(fit, il, type="response")
# Predicting the test data with model (old and new)
pld_old <- predict(fit_3, ilt, type="response")
pld_new <- predict(fit, ilt, type = "response")
hist(fit$residuals,								# draw the histogram of residuals
breaks = 50,									# number of breaks
main = 'Histogram and Density of Residuals',	# main title
xlab='Residuals',								# x-axis label
col= 'green',
probability=TRUE) # scale it to [0,1]. otherwise we cann't plot density curve of top of it
lines(density(fit$residuals),						# draw density curve of residuals
col = 'red'									# color it as red
)
y1 <- pld_train
y2 <- fit$residuals
df <- data.frame(y1,y2)
ggplot(df, aes(y1,y2)) +                    # basic graphical object
geom_point(aes(y1), colour="green")+# first layer
xlab("Fitted values") +
ylab("Residuals") +
ggtitle("  ") +
theme_dark()
y1 <- ilt$Total_Proteins
y2 <- pld_new
df <- data.frame(y1,y2)
ggplot(df, aes(y1, y2)) +                    # basic graphical object
geom_point(aes(y1), colour="green")+# first layer
xlab("Truth Value") +
ylab("Predicted value") +
ggtitle("Truth vs Predicted - Total Protiens") +
theme_dark()
library(Metrics)
root_mean_square_old = rmse(ilt$Total_Proteins, pld_old)
cat("--------------- Previous model with complete features-------------------")
cat("\n")
cat("RMSE test data with old model:\t\t\t", root_mean_square_old)
rmse = mean((ilt$Total_Proteins - pld_old)^2)
cat("\n")
cat(rmse)
cat("\n")
root_mean_square_new = rmse(ilt$Total_Proteins, pld_new)
cat("\n")
cat("--------------- New model with selected features-------------------")
cat("\n")
cat("RMSE test data with old model:\t\t\t", root_mean_square_new)
cat("\n")
rmse = mean((ilt$Total_Proteins - pld_new)^2)
cat(rmse)
cat("\n")
ninty.five.ci <- function(n){
ll.spikes <- loglikelihood(spikes, 2.5, 1.0)
true.count = 0
for(i in 1:n){
a <- generator(30, 2.5, 1.0)
b <- loglikelihood(a, 2.5, 1.0)
if(b > ll.spikes){
true.count = true.count + 1
}
}
binom.test(true.count, n, conf.level = 0.95)
}
ninty.five.ci(10)
ninty.five.ci(100)
ninty.five.ci(1000)
ninty.five.ci(500)
ninty.five.ci(5000)
ninty.five.ci(5000)
ninty.five.ci(5000)
ninty.five.ci(5000)
ninty.five.ci(5000)
ninty.five.ci(5000)
ninty.five.ci(5000)
ninty.five.ci(200)
ninty.five.ci(200)
ninty.five.ci(10000)
ninty.five.ci <- function(n){
ll.spikes <- loglikelihood(spikes, 2.5, 1.0)
true.count = 0
for(i in 1:n){
a <- generator(30, 2.5, 1.0)
b <- loglikelihood(a, 2.5, 1.0)
if(b > ll.spikes){
true.count = true.count + 1
}
}
typeof(binom.test(true.count, n, conf.level = 0.95))
}
ninty.five.ci(10000)
ninty.five.ci <- function(n){
ll.spikes <- loglikelihood(spikes, 2.5, 1.0)
true.count = 0
for(i in 1:n){
a <- generator(30, 2.5, 1.0)
b <- loglikelihood(a, 2.5, 1.0)
if(b > ll.spikes){
true.count = true.count + 1
}
}
binom.test(true.count, n, conf.level = 0.95)
}
ninty.five.ci(10000)
ninty.five.ci <- function(n){
ll.spikes <- loglikelihood(spikes, 2.5, 1.0)
true.count = 0
for(i in 1:n){
a <- generator(30, 2.5, 1.0)
b <- loglikelihood(a, 2.5, 1.0)
if(b > ll.spikes){
true.count = true.count + 1
}
}
binom.test(true.count, n, conf.level = 0.95)[7]
}
ninty.five.ci(10000)
ninty.five.ci <- function(n){
ll.spikes <- loglikelihood(spikes, 2.5, 1.0)
true.count = 0
for(i in 1:n){
a <- generator(30, 2.5, 1.0)
b <- loglikelihood(a, 2.5, 1.0)
if(b > ll.spikes){
true.count = true.count + 1
}
}
binom.test(true.count, n, conf.level = 0.95)[6]
}
ninty.five.ci(10000)
ninty.five.ci <- function(n){
ll.spikes <- loglikelihood(spikes, 2.5, 1.0)
true.count = 0
for(i in 1:n){
a <- generator(30, 2.5, 1.0)
b <- loglikelihood(a, 2.5, 1.0)
if(b > ll.spikes){
true.count = true.count + 1
}
}
binom.test(true.count, n, conf.level = 0.95)
}
ninty.five.ci <- function(n){
ll.spikes <- loglikelihood(spikes, 2.5, 1.0)
true.count = 0
for(i in 1:n){
a <- generator(30, 2.5, 1.0)
b <- loglikelihood(a, 2.5, 1.0)
if(b > ll.spikes){
true.count = true.count + 1
}
}
c <- binom.test(true.count, n, conf.level = 0.95)
c$p.val
}
ninty.five.ci(10000)
ninty.five.ci <- function(n){
ll.spikes <- loglikelihood(spikes, 2.5, 1.0)
true.count = 0
for(i in 1:n){
a <- generator(30, 2.5, 1.0)
b <- loglikelihood(a, 2.5, 1.0)
if(b > ll.spikes){
true.count = true.count + 1
}
}
c <- binom.test(true.count, n, conf.level = 0.95)
c$p.val
}
ninty.five.ci(1000)
ninty.five.ci(100)
ninty.five.ci <- function(n){
ll.spikes <- loglikelihood(spikes, 2.5, 1.0)
true.count = 0
for(i in 1:n){
a <- generator(30, 2.5, 1.0)
b <- loglikelihood(a, 2.5, 1.0)
if(b > ll.spikes){
true.count = true.count + 1
}
}
c <- binom.test(true.count, n, conf.level = 0.95)
c$p.value
}
ninty.five.ci(100)
ninty.five.ci <- function(n){
ll.spikes <- loglikelihood(spikes, 2.5, 1.0)
true.count = 0
for(i in 1:n){
a <- generator(30, 2.5, 1.0)
b <- loglikelihood(a, 2.5, 1.0)
if(b > ll.spikes){
true.count = true.count + 1
}
}
binom.test(true.count, n, conf.level = 0.95)
}
ninty.five.ci(100)
ninty.five.ci(10)
ninty.five.ci(20)
for(i in 1:200000){}
if(ninty.five.ci(i) > 0.05){
cat(i)
}
for(i in 1:200000){}
if(as.double(ninty.five.ci(i)) > 0.05){
cat(i)
}
for(i in 1:200000){
if(as.double(ninty.five.ci(i)) > 0.05){
cat(i)
}
}
for(i in 1:200000){
if(ninty.five.ci(i) > 0.05){
cat(i)
}
}
typeof(0.05)
ninty.five.ci(1)
typeof(ninty.five.ci(1))
ninty.five.ci <- function(n){
ll.spikes <- loglikelihood(spikes, 2.5, 1.0)
true.count = 0
for(i in 1:n){
a <- generator(30, 2.5, 1.0)
b <- loglikelihood(a, 2.5, 1.0)
if(b > ll.spikes){
true.count = true.count + 1
}
}
c <- binom.test(true.count, n, conf.level = 0.95)
c$p.val
}
typeof(ninty.five.ci(1))
typeof(0.05)
typeof(ninty.five.ci(1))
for(i in 1:20000){
if(ninty.five.ci(i) > 0.05){
cat(i)
}
}
typeof(0.05)
typeof(ninty.five.ci(1))
for(i in 1:1000){
if(ninty.five.ci(i) > 0.05){
cat(i)
cat("\n")
}
}
for(i in 1:1000){
if(ninty.five.ci(i) > 0.05){
cat(i)
cat("\n")
}
}
for(i in 1:1000){
if(ninty.five.ci(i) > 0.05){
cat(i)
cat("\n")
}
}
for(i in 1:1000){
if(ninty.five.ci(i) > 0.05){
cat(i)
cat("\n")
}
}
for(i in 1:1000){
if(ninty.five.ci(i) > 0.05){
cat(i)
cat("\n")
}
}
n.value <- c()
for(i in 1:1000){
if(ninty.five.ci(i) > 0.05){
cat(i)
cat("\n")
}
}
n.value <- c()
for(i in 1:10){
for(i in 1:1000){
if(ninty.five.ci(i) > 0.05){
n.value <- c(n.value, i)
}
}}
hist(n.value)
library(ggplot2)
ggplot() + aes(n.value)+ geom_histogram(binwidth=1, colour="black", fill="white")
ninty.five.ci <- function(n){
ll.spikes <- loglikelihood(spikes, 2.5, 1.0)
true.count = 0
for(i in 1:n){
a <- generator(30, 2.5, 1.0)
b <- loglikelihood(a, 2.5, 1.0)
if(b > ll.spikes){
true.count = true.count + 1
}
}
c <- binom.test(true.count, n, conf.level = 0.95)
c$p.val
}
n.value <- c()
for(i in 1:10){
for(i in 1:1000){
if(ninty.five.ci(i) > 0.10){
n.value <- c(n.value, i)
}
}}
library(ggplot2)
ggplot() + aes(n.value)+ geom_histogram(binwidth=1, colour="black", fill="white")
ninty.five.ci <- function(n){
ll.spikes <- loglikelihood(spikes, 2.5, 1.0)
true.count = 0
for(i in 1:n){
a <- generator(30, 2.5, 1.0)
b <- loglikelihood(a, 2.5, 1.0)
if(b > ll.spikes){
true.count = true.count + 1
}
}
c <- binom.test(true.count, n, conf.level = 0.90)
c$p.val
}
n.value <- c()
for(i in 1:10){
for(i in 1:1000){
if(ninty.five.ci(i) > 0.05){
n.value <- c(n.value, i)
}
}}
library(ggplot2)
ggplot() + aes(n.value)+ geom_histogram(binwidth=1, colour="black", fill="white")
library(ggplot2)
ggplot() + aes(n.value)+ geom_histogram(binwidth=1, colour="green", fill="white") + theme_dark()
